# Experiment 1: Comparing Soft Labels to SOTA Distillation Methods

This experiment evaluates the soft-label baseline against state-of-the-art (SOTA) knowledge distillation methods on CIFAR-10, CIFAR-100, and TinyImageNet datasets.

**Adapted from:** https://github.com/sunnytqin/no-distillation/tree/main

---

## Directory Setup

All scripts assume CIFAR datasets are stored **in the same directory** as the training scripts unless otherwise specified. This is not necessary when using the TinyImageNet dataset.

If you'd like to change dataset loading paths, edit `get_dataset()` in `softlabels/utils.py`.

### Dataset Structure

#### CIFAR-10
```
cifar10/
└── cifar-10-batches-py/
    ├── data_batch_1
    ├── ...
    └── test_batch
```

#### CIFAR-100
```
cifar100/
└── cifar-100-python/
    ├── train
    ├── test
    └── meta
```

#### TinyImageNet
```
tiny-imagenet-200/
├── train/
│   └── [class_dirs]/
│       └── images/
├── val/
│   ├── images/
│   └── val_annotations.txt
└── wnids.txt
```

---

## Example Commands
Commands are to be run from within the directory that the script is stored in.

### Train CIFAR Experts
```bash
python buffer.py --dataset=CIFAR10 --model=ConvNet --train_epochs=164 --num_experts=1 --buffer_path=results_10_F --data_path=../data --save_interval 1 --lr_teacher=5e-4
python buffer.py --dataset=CIFAR100 --model=ConvNet --train_epochs=125 --num_experts=1 --buffer_path=results_10_F --data_path=../data --save_interval 1 --lr_teacher=1e-2
```

### Train TinyImageNet Experts
```bash
python buffer.py --dataset=Tiny --model=ConvNetD4 --train_epochs=102 --num_experts=1 --buffer_path=results_10_F --data_path=/scratch/np5n22/tiny-imagenet-200 --save_interval 1 --lr_teacher=1e-2
```

### Train CIFAR Students
```bash
python train_student.py \
  --dataset CIFAR10 \
  --data_path cifar10 \
  --expert_path nozcacif10model.pt \
  --ipc 1 \
  --use_soft_labels \
  --student_model ConvNet \
  --teacher_model ConvNet \
  --expert_epoch 20

```

### Train TinyImageNet Students
```bash
python train_student.py \
  --dataset Tiny \
  --data_path /scratch/np5n22/tiny-imagenet-200 \
  --expert_path tinynetwork.pt \
  --ipc 1 \
  --use_soft_labels \
  --teacher_model ConvNetD4 \
  --student_model ConvNetD4 \
  --expert_epoch 11

```

---

## Command-Line Arguments Explained

- `--dataset`: Name of the dataset to use. Options: `CIFAR10`, `CIFAR100`, or `Tiny` (TinyImageNet).
- `--data_path`: Path to the dataset directory. This should point to where CIFAR or TinyImageNet data is stored.
- `--expert_path`: Path to the expert model parameters file (a `.pt` file containing flattened parameter trajectories).
- `--expert_epoch`: Epoch index of the expert to load. Use `-1` to automatically load the last epoch.
- `--ipc`: Images per class. Number of sampled training images per class.
- `--use_soft_labels`: Flag to indicate training with soft labels generated by the expert. Need to use this when running - hard labels not implemented.
- `--student_model`: Architecture of the student network. Options include `ConvNet` (ConvD3 by default), `ConvNetD3`, `ConvNetD4`.
- `--teacher_model`: Architecture of the teacher (expert) model. Should match what was used to generate the expert parameters.
- `--epochs`: Number of epochs to train the student model.
- `--lr`: Learning rate for student training.
- `--temperature`: Temperature for soft label smoothing. Higher values produce softer probability distributions from the expert.

---


## Iridis Launch Scripts

All scripts used to run this project on Iridis are included in .sh files.

## Contact

For questions, issues, or contributions, please open an issue or contact the project maintainer.
